Nalini: And all right. Yeah. So we have a very simple sort of mutual prepared, but the idea of that is just to guide our conversation and at a high level context. The reason to set up this meeting was, we had our initial tech understanding conversation with both of yourselves as well as with about a week and a half back. and the idea there was to get a quick capture of what is the sort of tools that are being used and a little bit of insight into what is being used from the data perspective. And after that we've seen some of those tools we've seen at least a little bit. We haven't had a chance to play around with it. but we've also had conversations in No. We still don't. and that's something we are following up on. We were rather following up with Charlotte and Pittometry as well. So we did have conversations with them on Thursday, and there is sort of an outstanding item for theometry to create that for us. But yes, short answer. We don't have it yet. Let me ping them at 3 while we are talking. So you find that another bit of context they're just adding, Not only we also have an outstanding item right to to get to Dmitry the list that he's requested with the with the proposed business. Alto-falante Desconhecido a business case for access. 

Sefa: Okay, does that include AI as well like as a part of the logic. 

Rahib: Yeah. So at the I we did have it as a verbal conversation. Antimetry was good with that right? You missed that on Thursday. Yeah. What we agreed was that Atl AI Tommetry would go ahead and create a. 

Nalini: you know test, not test, but a sort of tummy that works right login for us without 2 factor. so that we could go ahead. And yeah, for the rest, we were. We are due to send a list to theometry to get any other kind of access. 

Sefa: Yeah, okay. Not because I'm good to send the message through. 

Nalini: Yeah.So with that, I think in the last 2 and a half 3 weeks. We've sort of gathered bits of context which has begun to form for us some sort of an understanding of you know what's the current data need. What's the potential future need? And so on. The idea in this session is not the solution. Of course it's to just explode that a little bit more. But again, with the intent that we want to get some sort of a background picture of that landscape getting built up as well before we dig deeper into what's going to be prioritized for in the immediate 3 to 6 months. And therefore, how should one actually solution for data for that as Well, right. So this is kind of still in the realm of building up that background. So with that I want to start sharing my screen. 

Sefa: Sure. 

Nalini: my screen visible to everyone. 

Rahib: Yep. 

Nalini: Okay. So very quickly. And this is lose agenda. You know. We've got about 55 min. And we don't have to go through all of them. We just kind of want to get started on some of them as well, at least. so to start with, I want to do a bit of a playback on what we've understood as a data flow diagram to you based on what we've heard. And we've taken the liberty to sort of extend some of those to a more generic understanding of operational data and analytical data. And we want to play that back to you safer and just to see if it resonates, and whether that's a good way to begin thinking of that. Now, whether that's a first priority or not, that's a second question. This is just to play back what we've heard in the context of how one would typically think of a data platform. And then we with with that, you know, sort of under way. We want to capture very specifically the flows for telegram potentially for the Gsp video script as well. I really we want to do that for CLI AI to, though I mean not the notion. That's kind of scratch notion from here area as well, though I think previously you've told us that that's not available with with you, with either of you right now, and we'll have to wait for the fifth trip download. But that's the direction in which we want to go. So if we capture a little bit of from a current State perspective, what's got built up in the data landscape. and then, if you have time today, or we'll set up a follow on conversation. I find Gil and me can tell you a little bit about what's the kind of spike that they are doing on notion. And again, right now it's more, you know, kind of theoretical, because we don't have access to the notion, Apis, that you have. but just based on what they've been reading, and what the line of thought is, you know. At least we want to share that it's, too. What are the factors on which we want to spike. so that one can start thinking of notion as a data source in the overall solution? I mean, that's the direction in which we want to go right, and this would again very likely not come up today, but it'll. It's something which we'd want to get to eventually that what in the data platform needs to get prioritized. So with that, I want to jump in here and just walk you through a little bit, and please feel free to pause me, or let me know if any of this is not clear. But this is more of a playback, and the idea of the playback was to think about at a high level. What are the main processes? And again, this is simplified. But if on things with the presses. You know there's a whole process of finding research interest and based on that. There is a research interest, raw data which then has to get kept in some way, and that's that's sort of collated in some way. And then in some sense that's both stored and modeled upon before it can get visualized. So this is a very sort of simple typical flow. and at least in our heads that's the flow that's there for what we broCLIy call as operational or transaction data as well, which really means that this is large volume moving data which is captured, stored, and analyzed, and visualized or reported on. but used largely for more day-to-day purposes, and not really persisted for a longer term use. And in this sort of flow. We see tools like flashpoint, and brand watch, and I guess, seen analysis as well. which are proprietary. But they would follow this kind of an overall process, except that a lot of this is, you know, sort of with them right? I mean the storage is with them. The dashboard is also with them. and we have access to some of that through Apis. But at the same time this follows there, and they are the ones who are scraping. Let's say 20 sources. So there are upstream data sources that they are scraping, and then they are following some kind of a logic to select Some of those, you know, probably run some Ml. On it, and then visualize it. and they resp. have their own stores as well as their own models, as well as their own visualization. But they do expose api's for us to capture from that Now, on this similar flow there are also a couple of custom tools that have got built in-house. So the telegram tool is an example, the Gsp. To this, an example which also follows some sort of a flow which is similar to this, where some of it is captured. There's some kind of model done on it, and there's a dashboard which visualizes this information. The monitoring flow is essentially an update on this. So we've put this down as sort of an operational space. So i'll pause there just to hear, you know, sort of reactions or comments before we move on to what would then sort of serve as an analytical space. 

Sefa: first posted like the L. Of note, and then the second one is for transactional data and build model. Are we talking about the data model, or like a machine learning model here? 

Nalini: So this is, yeah. So this bill model is a machine learning model. The store. Transactional data is more of storing it. It could be modeled well, or it could be, you know, very loosely modeled. But yes, it is. It is an explicit storage in some way, and then it is building. In this case a model, or it could be any other form of transformation or analytics. It is more to show that there is that sort of storage plus processing that happens before one can look at how you'd want to visualize it. 

Sefa: Okay, that's cool. So here. not sure if we need to like. Go into this now. But the emphasis can be only a bit more about data, less so with like training a model, because you know, it's not like a. you know, very heavily model producing entity, and especially like considering our current and upcoming partnership. So we might offload some of this work to third party. So this could be like Api level enhancements rather than, you know, training our own models, because. considering the bandwidth. And you know the priorities. We build models where we cont or float the models like that. That is like our, you know. Not. First go to model here. Our first go to is like. Can we find something approximate the need here? So, for example. you know, open source libraries for basic tasks like any are. Or you know, third party vendors that actually provides them as an Api. like partner with people who's worked on this and what not, because Agency doesn't have, like a great track record with like model building as a as a product. So therefore, we approximate it as more like a project, base, or, you know, do if we don't have anything else. But in the future, obviously, we can revisit that. But currently, yeah, we we we tend to. you know, partner with other folks, or use third party apis for this work rather than building everything in-house as and when needed. 

Nalini: Yeah, yeah, fair enough. And I think I would, you know, almost extend that to say that that sort of bill plus by decisioning is available at all of these touch points. So. for example, S. 

Matt: maybe a buy option that one wants to look at here from a data source, perspective itself. because there is some additional model that you want to run for some insight that you want to generate, which none of the other services provided to you as opposed to let's say a flash point which is providing to you the whole sort of flow. But you may also go to the Api and pull some specific visualization that is not, you know, easily available. So I think that sort of bull by combination is available at each of these touch points. But at the at the first level, you know we wanted to validate that fundamentally. This is how it's working, and the approach is that wherever you can buy you will want to buy. and you would want to build where either it's not available, or there is a clear reason for differentiation. and there is a longer term sort of strategic purpose to it. because of which there is merit in investing in a in a build. 

Sefa: Yeah, correct. And again, yeah, we can. We can like talk about strategy going forward. But this is yeah, that this tracks. But i'm not what i'm not seeing. Here is maybe this like a separate stream. So this is very observational, right. So this is very, you know. Dashboard the solution. So where do we get this? More like hands on platform solution, like because social media monitoring is only part of our workflow. Right? So it's only helpful if we can tie it back to. You know the cases that we are looking at like individuals and whatnot. Is this just like they, when you say data flow? Is this like social media monitoring side of things that can serve for discovery a bit? But once you know what you're looking for. How do you draw back from this like this? What! I'm not saying that the hands on interaction side of things for researchers. 

Nalini: right? Right? So we've separated that out a little bit as the analytical plane, and that may or may not be the right way to represent it, but just one kind of you know, play that back to you. So this is more in the more in the more typical realm of what one would do with operational data. Right? So this is your investigative researcher, your investigative, and maybe the research assistant, or you know, the different levels of investigative researchers who are doing this day to day work and looking at all of these sources to do this sort of analytics. But there is also an aspect of. There is a certain part of this data that you then want to curate and retain for a longer term time, period, as well as over a longer term time period, and you want to do that for analytics which are of a different nature where you want to combine some of this, but also it's not to give you an immediate alert, but it is also to give you. maybe trend lines right, or it is meant to give you interconnected information between different touch points. So here we look at what what is called sort of recorded as, or what is called tagged, not just required for it's got, and this doesn't sort of, you know, imply the volume at all like this can be very large volumes as well. But data, which is in some way got tagged as data which has to be curated is data which is then going through this sort of a more analytical workflow where you are saying that you want to categorize or standardize that in some sense, because that's the only way you'll be able to kind of pick up that data going forward. There could be some kind of aggregating or slicing of that again, based on how you want to use it. And then there is still again reporting and visualization of that. So there is that step as well. And in this, in this sort of flow, from whatever we've heard so far. we have this only in bits. So we've heard bits of this in what is got built as an CLI AI or notion as well. This is not complete, but you know there's a sort of work in practice in the aggregating and slicing of that sort of analytical insight. We've seen very few examples in what we've heard. Potentially. The heap map is one of those, and potentially the process of aggregating for the heap map, even though manual is in that sort of logical bucket, and therefore there's a lot of white space in what has to be done here. But I wanted to play that back to see that as needs. And if that sort of covers what is needed from the perspective of ready to landscape. 

Sefa: Yeah. that's correct. So again, like the agency has not been producing a lot of data that we can draw on. So therefore. the focus on social media data was mostly for that. We don't have, like great internal per processes that actually generates our own data. This is why what used to be Thel AI, which is now transforming into notion, or whatever, like salesforce, the case management of the data we produce is going to be key going forward for this side of things like we have not done a lot of data science or on on our own data, because that data lived, you know, in tens of different places. So it's her bitcar is talking about I 2 today which has its own database exported, but it doesn't live in a way like was an effort. But I wouldn't say it actually captured the totality of centers data. I think there is like gaps if we only work with like a and notion. So the and the gaps being like you wouldn't have any historical data at all. So you can't really provide, like the past context, or highlighted to future researchers, but it might be relevant and helpful. So we have like 

this: this tracks right. So aggregating and slicing resource data. And also I want to flag that you know this as a platform. The platform can do certain parts of it. But our like internal data. Science team can also take over some more like custom hands on analytics and build on top of the platform. So the as long as the platforms data. Is it exposed to us, right? So maybe, like. you know, the standard design. our design standard, the size things can be done through the platform for day to day, and you know, maybe even like some summaries. Alto-falante Desconhecido for, like larger trends, more like yearly reports and stuff, we can do like 

Sefa: more like rather than trying to automate and trying to force people to make sense of this. That so some feedback I heard last week doing retreat is as the analysts like the analysts. Don't want to understand what the trends are, what an outlier is. What is the spike like? Meaningful spike is, or the comparison, like trying to sense of that comparison, like basically generic data science workflow where you produce insights like, I don't want to deal with like statistics or on Time Series, data, exogenous factors, or, what might be. you know, constraint as a interesting signal, where which what might be considered as a as a noise like that might be. I'm not saying we're going to put this to our analysts, but like something. Maybe we can think about what is built into platform, and what can be built on top of the data we generate with the platform and can be done like AD hoc reporting on the site. 

Nalini: Yeah, yeah, yeah, no, absolutely. I think, from what we've heard right. what's available in this whole sort of space is fairly limited at this point, and even if one you know, builds build something like the heat map of a platform, and with automated steps of educating and slicing and visualizing, and so on. And once one has that platform in place. It's quite easy to have more standardized reports based on for what audience, what is useful to see. But you're absolutely right. The whole power of that should then be that it allows for a talk, reporting. As well be that rule-based or be that using amel techniques, or whatever that is, but it should make both of those available and of reports. Let's say you have a reporting to like a taboo, or something else that's kind of attached to the end user it as well. They should be able to do AD hoc reports. which are just simple reports, so that should be possible. But I think the point that I was hoping to highlight using this playback was to sort of situate what we have currently within the what's a more operational set of plane and analytical sort of plane. and to highlight that the purposes and the end users of these would be different, and therefore trying to do both, and everyone does both right when I say trying to do both. It's more of a priority question. But how does one sort of you know, balance the upcoming early investments on adding more here versus strengthening the foundation here. because it's also that if I has to do this process. there is a cost associated with it again in it's no longer the cost of data, because the data data is very cheap in terms of storage, and so on. But there is a cost in processing this in an effect to be so, there is a cost of cataloging it. So there is that you know, sort of intelligence that has to be applied to it, using human presses as well. Let me so much. I can be medicated. So I think you went to Mark's question today.NalIt's not just about a data, and 3 person who can come in and put in stuff. because then it's, you know, like garbage and coverage out kind of a thing. You can do that for our data, or you know you can automate that. It's a lot easier. It's going to be a lot easier to automate that as much as you can. But to do this sort of curating you would actually cataloging. You would need to have some set of oppresses in place. We can make their presses easy. but there would still need to be that process so that one can get to this point. So I think that's what we would want to play back. And you know, kind of get reactions on as well. 

Sefa: Okay. Yeah, this tracks and I believe the data sources can be. you know, changed, increased, decreased, depending on the priorities of the day. Right? So it doesn't have to be like Alto-falante Desconhecido setting stone, as is currently. 

Nalini: absolutely absolutely so. We didn't, you know. This is not. Maybe we're not ready to show this right now, but over time. What we want to do is build a bit of a functional view where all of these one can think of them just as data sources. Right? So notion is also a data source. Area is also data source salesforce. The data source as well as smart would be data source. So one can think each of those as a data source. Some would be a data source which comes from a data provider. So it just comes as a stream of data. In the case of something like notion, this is a transaction system in itself. The notion is not only providing data, but also updating data, and therefore there has to be some process of so notion also has a whole case management user interface, which is interacting and so on. But effectively, from the perspective of the data platform I can just treat all of those as upstream data sources which come in and different formats at different regularity and for different purposes, and then sort of treat it. They don't. But so, coming back to this, so with that, said I'll, I'll just pause if I don't have any comments, or if how far you let me have anything before we kind of step into. I think, the next area that we wanted to go to aaron I have a question about the analytical stuff. So are those is a is a way to think of those like applications that we're building on top of the data, so like the hem as a product that we're building. using the data that's taken from the operational thing. 

Nalini: Yeah. So what we are thinking of that is, is this. And this is, you know, fairly typical classification that we're kind of trying to apply in your context. But the whole idea being that Yes, when we're talking about data in the analytical plane, it's data, which is much larger volume, much more historical. but at the same time categorized in such a way that in send somebody categorize in some ways, so that it has long been of purpose, and on that yes, you would apply some kind of an application, so that you can see it so so far. What we've heard of as an application is the heat map. But definitely fun is investing to build this. It's not worthwhile to do it just for a heat map, so the idea would be then to go out and search for those used cases, and the almost the you know champions for this to say that who's really missing this? And for them we'll build up those applications. So the heat map might just be the first one which is already out there, so you know. How do you automate it? But the power of this would be, and which also prints me back to, I think, what we've been hearing a little bit on CLI AI as well as notion. Right? Everyone's kind of talking about. We need to store this efficiently absolutely, You know. We need to find a way to store it and at least categorize a minimum level. inefficient way. So that's not a pain point, but that data doesn't have use if it's only meant as a store which can be accessed. 

Sefa: which is again right. It's not just an area to be searched. 

Nalini: It has purpose. If you are then using it as cataloged data which can then be aggregated, sliced, modeled. and so on. So it can be visualized. So I think, yeah, these steps are largely missing at this point. I think the focus is up to here mostly. 

Rahib: I just sorry. Go ahead. 

Sefa: So I just. I just like heard from Dmitri. And then he says he emailed you folks for atl AI access. So, and he's going to make sure it's working. 

Rahib: Okay, Got it? Yeah, I'll i'll go through my inbox. I don't think I've perceived anything like that. But but i'll i'll definitely I can reach out to him personally to on the it's like here. But 1 one thing to get back to this point here, right and kind of what Aaron was mentioned. The concept of data products right? It's not just the implementation of what it might look like. It's really from a production and a consumption. Perspective is designing and building these products data products, and how the operational aspect of it feeds into the analytical part. So there's a bit of applying product thinking as well to to how these data you know. Data entities are are created into products. 

Sefa: So one thing that I came out came away from the Our retreat is that i'm wanting to. you know, form like a data consumers group with some like mid-level managers, so that people actually consume data that we're producing so be it like through social media, and just be through like enhancements, or just like data entry from day to day. So that is something that i'm going to try to like form together. So. And then Alto-falante Desconhecido in that format, like, we can idea together whether that becomes a 

Sefa: what, what I would form. It Shapes like that will be an internal working group of sorts that I want to form like. Yeah, we can actually do more like analytics. And you know what like, for up to questions is like what your folks have been looking at your when I when I say your say, like East Coast Irs have been producing in the last I don't know, like month, or what has been moving the needle for idea of Coe for this last year. What has been the trend of information that's actually, you know, tracking like. Is there interesting patterns that that you wouldn't have come across individually? But when we aggregate the data, like certain groups, become a more reactive, more interactive, more like cross, pollinating among each other across ideologies and what not. That sort of things we can build on top of a good case management slash like social media monitoring, which is something we don't have to. Yes. 

Nalini: yeah, yeah. And I think it's a little bit also on you know what's the question that the analytics is trying to answer. So, for example, if the question that analytics is trying to answer is for a particular investigation that's going on, are we able to connect all of the different information that we have, so that you know we can proceed and sort of look for the next source, and so on. then. Yes, you know it'll fall more in the realm of an operational analytics. And how does one, you know, sort of figure out what to do next in the next day ours and and you know so and so on. But if it is broader to say that you know which regions are we tracking over time, or you know what what sort of partners that have you been able to give information to, and what kind which is actually led to clear action, right? Or what sort of data sources have been most effective. Because I don't know if you, If you are aware right now, you know, with the information that you have that a telegram versus let's say, a twitter versus let's say you know what you're scraping from the Jsp Transcript. What's most effective because that sort of tunes towards a more midterm decision. So those would come into this sort of an area very broCLIy. But yes, the idea is to find out what applies, but more importantly, also to get people on the same page that if you want to look at that data more analytically, then one has to do these processes and invest in it. And again, some of it is being talked about. But then it has to be used right. This shouldn't be used only as a searchable store which used as something that you can draw out analytics from. 

Sefa: I would also. you know, like prompt us together to think about. You know how, if I'm, all the data is right. So if to operationalize data. if we are thinking is not to store it in a long term space. so that might have its own challenges attached to it, because our analytical processes are not really well established. Right? So we are not like putting together kpis and whatnot. But we want the data platform to serve investigative research. So and if we don't store some of our, maybe all of our operational data for the long term and make it search, plan analyzable. There might be some drawback. So, for example, so give me an example. So a guy on telegram just posted a threat. Right? So and then we detect this with our email tooling or keyword search and whatnot. and we want to pull all the messages from that guy. So if you have not been like looking at that historically like one key inside we have is that these extremists can be a bit sloppy when they start right, so their first messages might be illuminating to give us some sort of idea where they live, or they might post more personal information from them. And another thing is, they might actually have changed their at handle, or we might not have some sort of you it on them just to have like some sort of add handle. and that at handle. My. you know, caught their messages from a certain point onwards. So one insight, or one like. Again, i'm thinking about the researcher research platform sense rather than and in an ethical trend based platform sense is that, you know, if a guy has changed their handle, so maybe the earlier handle will allow me to do more more research around that individual. So maybe they use that, and they'll in their email, which I can then fade into a search engine and then search if there's Gmail account attached to this thing. So in my right and understanding that we say the operationalize the operational data is not going to be 

Nalini: stored. So we're not throwing it away right? It's still going to be some sort of a variable, unlinkable. Yes, yes, absolutely so in a case like this, the one that you described right, which is telegram, and which for which is a custom sort of path absolutely. There has to be a storage layer here and there would be definite parts of that storage which then is persisted from a analytical perspective as well. Now whether this is move, but it's the same store. You know. That's a different question. That's a solution in question. But definitely part of that is going to be stored. But the idea is that for an operational purpose you may store all the information that you get. whereas there is a prioritization of of that which you want to store for a longer period of time. If you want to think of it from an analytical purpose, and that's a decision. That's a decision that can be made as to what what is worthwhile to keep. But that would be true of any system, right? I mean, this is in this case. You have control on it because it is a telegram, and you are searching through telegram yourself instead of storing it. In the case of something like flashpoint. For example, this is slightly opaque to us in flashpoint. How long they store this sort of transaction latest. We are not aware of that. But instead. 

Sefa: there's certain insights that we get, or you guys that we get. And then we sort of see if that into this stresses in some sense as cases. Okay. So one other thing that we should be thinking about, especially when we don't have like, we, we can design the stuff we scrape for long term storage. but for like Api access to things like I don't think Brand, which will share in data, by the way, and i'm not sure about flashpoint, because that's their business is. you know they will share some. But if we want to replicate all data platform, like all data on our side. I'm not sure how they will react to it by any who like. What I was trying to say earlier is like, maybe Flashpoint has, you know, valuable data, but like the enhancements or the searches, etc. We want to do on our site. So that's great if we can. you know, query it on at request. but especially for a male tooling. you know, driven enhancements. There might be some replication we want to do on our side. So again, like, doesn't maybe have to be thought about at this level. But I Alto-falante Desconhecido that is going to be something of interest as well. 

Nalini: Yeah, yeah. Yeah. So in fact, I think that's a great segue into what we wanted to do next, which is, basically go through your telegram, scraper and up to the dashboard flow just to get a sense of. What are we really talking about here right? And we wanted you to describe that and capture that for exactly. You know the same reason why what you just said. that there is an operational store as well as there is a operational data volume. format and velocity that we need to understand as well as we are building the product platform. So that's actually what we wanted to do next. So we wanted to start off with doing this kind of a playback, just to see if it resonates. and then also retreat, that this is the part which is developed a lot more right now, at this, to an extent there is very limited. and we want to keep that in our background. As we sort of figure out how we prioritize this. Are we going to have like a separate conversation around case management? We would. But I think we want to get at least some access to a real AI, or to the notion, Apis. and understand that a little bit more. And then then do that with you. 

Sefa: Okay, maybe not now. Okay, because case management on itself, like as a consumption or product to be consumed. Can we be important to folks like all the trends and whatever they will be supplemental? But like entering information around the individual and playing it back. and maybe like seeing what you know is on telegram or about that individual on the data platform. I don't know how you are conceptualizing that consumption like day to day. Consumption around individuals is good, but that's going to be a key. So normally it's approach was like you enter, and then you click on individuals, and then you see whatever you enter it, plus other linkages. I'm not sure if we can replicate the same thing on motion, or really going to think about, not only just like a data entry point, and then the platform is going to be for consumption. or we're going to feed back data into notion. So that notion we'll have that. And then the you know data linkage stuff like seeing how a case around the individual has evolved over time. So this is a data consumption problem, right? So people will consume it. Maybe not in a way that you know it ends up being in a heat map, but like how our case around that individual has evolved, and then being able to see that is interesting for multiple things. So for an individual to see what like for an Sm. To see it, and also for a manager to see it, maybe at a higher level. What methodologies are actually moving? Right? So what methodologies are working to move the needle for us. and being able to articulate like an interesting case where we drew one of our data products to, you know, make a case around an individual, so that Alto-falante Desconhecido lineage and record. 

Sefa: I'm thinking some sort of like. you know. not like like extremely a get workflow, but like capturing changes over time. Whether that be changes to case management, or, you know, like new data linkages around the poi as well as discovery on that platform like that's maybe like a further down line like a ux, you question. Alto-falante Desconhecido But like. 

Sefa: if you need to think about this at this stage I wanted to flag them to you as well. 

Nalini: Yeah, yeah, no. I think that's a fact. Call out. But you know also I I I my. What I would also tend to is that it's a ux question, you know, quite front and center as well, because from a data platform perspective we can treat, I mean in theory, CLI AI can be treated as a historical case system. And let's say this idl AI plus. There is I 2, as historical case systems, and there is notion as a current case system, you know, just just theoretically. And yes, they are all not seamless together. and the user interface for that could be a notion plus plus, or it could be a completely different user interface which combines all of these. But yes, you know we would. We would treat that as a a ux and application perspective. Where is from a data perspective? Each of these can be treated as different data stores which have the same kind of data, which is case data. 

Sefa: Okay, that's great. And also when we're thinking about. So it is. What i'm not seeing here is like like Lexisnex is of the world right? So external Api calls, and generating that data on the fly. It doesn't have to be just lexisnexes. But like other tools like that. less tangible. but also going to be like drawing people back to data platform, because that is, data that we don't persist ourselves or not, queriable or like not as credible as like we don't know if it exists or not. It's just like an Api call. So where that would sit within this chain. 

Nalini: Yeah, I'

m: I'm just gonna make a note of that, so that we incorporate that. 

Sefa: So I was not talking about Matt more like lexus nexus right? So getting information around, and or like criminal cases, database or property database, so these are like all the external Apis that we want to connect to, especially around the individuals, or maybe groups. or like what we have here, is like a tax office Api, where you can put a name and then see their financial assets or their interest in companies and whatnot. Yeah, like a search top. Yes, exactly. I'm not sure if it that sits within data, but I feel like it should, because we need to have an understanding what data we can. You know highlight for our folks. and on pi or or other things, because again, that's not data we hoard. But we might decide to hold going forward. and that will be more purposefully collected. Collected data. 

Nalini: Yeah. 

Rafa:  I like that. I I I so sorry. You know you need to do that. Speak? Sorry. I like the idea to have a a a session, because we can collect the data that came. They requested the response, and they request itself. What the users is searching. What do you that? What kind of statement you using to get this content and some kind do some kind of correlation between them and discover account of research is important. 

Nalini: Okay. So yeah, and I know we have this about 10 min left. So if it's okay, we'll probably just segue into one of the flows, which is this kind of a flow, and just take some notes on what is being done currently to, so that you know we have for the questions of that kind to ask. So maybe we can look at telegram. And these these are the kind of questions that we want to ask. just to give you a trust. You know it's not. We don't necessarily have to go in order. But what we want to use is that if you are using, and And this is actually for the video that I put down. But yeah, let's say that even from a telegram perspective, when you look at the source. we want some sense of how many groups are accounts? Are you scraping? How long back does the data go? You know. How are you storing that data? In what format as well as what is this to that you're using? And are you? You know, sort of Are you asking it in some way. and so on. So that's the direction which we want to go, and i'll probably just take notes if you can walk us through what's happening with the telegram data flow. 

Sefa: Okay, so you want this for telegram right? aaron Sure. you ever just describe the floor literally. Just go through the questions on the right. 

Nalini: Yeah, You can just describe the flow, and we can ask you, you know. aaron Yeah. So right now, I think we're scraping 6 or 7. 

Sm: me accounts so essentially these are accounts that they have signed up for telegram accounts, and they follow channels or groups through that account. and we are connected using an Alto-falante Desconhecido the telegram Api to ingest all the data aaron from the channels and groups that they have joined. So the advantage of that is that if they join a private group or channel that we have access to that data as well. 

Nalini: And just to pause you would you say that if it's 6 to 7 Sme accounts? Would you say that the number of groups, such as are in the 45 range is in the country range just enough rough sense, a rough estimate I think it was like a 1,500. Okay. 

Sefa: chats and channels. This is not just chance that we call them groups together. 

Nalini: Yeah. aaron Okay. yeah. So we're ingesting that data hourly. when, when, whenever we add a new Sme account, it goes, and it imports all of the message history for the channels and groups. So all the way back as long as the do is not to leave it on telegrams. And and then we ingest the last 30 days of photos. And right now we're not doing videos. So say I added my account today. It would in just all the messages, and then the last 30 days worth of photos, and then all photos going forward. But no videos. 

Sefa: And these are these are customizable by the way, on on requests we don't in just videos for storage and speed of ingest purposes we can, but it has been requested. So we're skipping basically 

Rahib: and sorry. This is all synchronous, right? There's no recirculus methodology in place at this point. aaron Yeah. Outdates. Yeah, synchronous. I mean, yeah, technically, the the Python library that we use to connect to telegram telephone. It does look at some acing stuff. But from our perspective this whole process is running just one step at a time. So the first step is like connecting to Api, and all that data is saved to S. 3, I think, as Jason files. Alto-falante Desconhecido And aaron And then there's another step that actually gets the data from S 3, and then put it in the database. and what database would that be. It's a post-graduate piece right now Alto-falante Desconhecido it's. Also aaron we're also replicating some of it for analytical purposes and data bricks right now. 

Nalini: Okay. And once you have this data, can you describe the further processing that you're doing to it before it comes up in the dashboard. aaron Yeah, we do some data enhancements. So so for set up the the messages to be processed by open AI, and also. So what was Google'

s: yeah, like we, we have a few text class fires that's running on data to score 

Sefa: the messages for different different ontology. So it could be threat. It could be violence. It could be identity, attack all that sorts of you know. or hateful and whatnot. So we also label the images for objects in them. So we are mostly well, almost exclusively highlighting. The The weapons are like violent stuff currently on our telegram dashboard, which you have seen, which is the like. We have a few telegram dashboards, but one of them is to help with like daily monitoring. But the other one we have is just like in operation a lot like Sorry I I can't share my screen, but I more like a summary of sort like how many data points we have so currently we are at like 26 million telegram messages and the scanning created as we on board more smes. So the the thing is like we look at only a semi accounts. Right? We don't expand. We don't do proactive data ingest which we can or may in the future. But currently, the data is ingested only from Sme accounts using a Sme account. So this gives us access to private spaces that if Sam is get in there we are in there, but otherwise we're not like following huge number of the or at least we're making an effort to follow, you know, like we can easily in just 20,000 groups. But we don't want to do that unless there is a reason to do so. So that might be a bit different for a Flashman, because they are trying to serve more like a journalist audience, whereas we're like more coe specific. But if an Sm. He joins a new channel, so next in just round, which is the next hour. So we, you know, pulled messages from that particular channel or chat so the transform. Sorry i'm gonna throw it back to I and for the transformations on data breaks or elsewhere. 

Nalini: If you could explain what you're doing with data bricks specifically right now. aaron So right now, that's preset, as we will, the tool we use for dashboarding. And right now Preset is connecting to data breaks. Yeah, that that was essentially just done to speed up. Then it'll political side and also release some of the load on the postgres server. 

Nalini: Okay, yeah. So you're doing a the application here. aaron Yeah. 

Sefa: not full replication. There is some sort of Etl there. aaron Yeah, it's it's a combination of both. We. I believe we do have full replication, but I was talking about more like it the dashboard sort of things. Yeah. 

Sefa: So go ahead with the Etl. aaron Yeah. So for the dash for some of the dashboards. Sometimes we're just using Redx search. So inside data breaks we have a notebook running that's looking at the same day in bricks, data, telegram data and then running Red X or searches so that we can show those on the dashboard. I think, my, we might also have some legacy. A legacy notebook that is looking is doing like the rejected search or full tech search. and the postgres database. But that is being replicated to data bricks, just to also speed things up and relieve the stress on the post customer. 

Nalini: Got it? Got it? Oh. fair enough. I don't be it open. Talk of time. I miss the questions open and perspective for this. Okay 

Sefa: has a question, I guess. 

Rafa:  Yeah, about the the injection process. How how do you guys do this today? You some kind of instance on Lambda are running and calling the Api how this works actually aaron right now this is all running on airflow. So the airflow is being used to connect the Api telegram Api, and then save the data test 3, and then also load the data to the postgres database. And there are some notebooks and data bricks that are being used to. or you the data in Postc, and then persist. And 

Rafa:  okay. aaron And for the full replication, Cdc. we're using air Byte 

Nalini: fair enough. Yes, I think this gives us some bit of a picture of what's the tech landscape, at least from the perspective of what you've built for telegram. What we'd want to do in a following session is maybe quickly capture this for the video transcript as well. and as soon as we can get some handle on the on Atl AI, We'd also want to at least play back. Some of what we understand is, where are those cases stored? Because that's again sort of a different kind of data set. 

Sefa: but we want to send that as well in the scheme of the product or a data platform. Alto-falante Desconhecido You don't have a lot of like visibility on that. But with this platform we're building together. We actually want to have more visibility on our on data. That's something we can discuss later. 

Nalini: Sure, Sure. sounds good. So yeah, we are almost on time. But thanks a lot safer. And Aaron, and we are going to kind of reach out to you for another slot. Maybe it towards the end of this week. and we'll have sort of a part, 2 of this 

Sefa: awesome. That's great. Thank you. aaron Thank you. 

Nalini: Thank you. Thanks, everyone. 

Rafa:  Thank you. 

Nalini: Bye.  

Gui: There you go.